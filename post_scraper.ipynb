{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extractor for Satoshi Nakamoto Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract links from main site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use requests to grab website raw content\n",
    "posts_rawlist = requests.get('http://satoshi.nakamotoinstitute.org/posts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup to parse/format the request information\n",
    "soup = BeautifulSoup((posts_rawlist.text), \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the div containing the 'li' with the post links\n",
    "list_items = soup.find(class_='col-sm-6 col-sm-offset-3 col-md-6 col-md-offset-3 col-lg-6 col-lg-offset-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the links in the above div\n",
    "link_items = list_items.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list for holding our links list\n",
    "links = []\n",
    "\n",
    "# extract the href content, add the root URL, and place the post URL into our 'links' list\n",
    "for link in link_items:\n",
    "    if link.has_attr('href'):\n",
    "        links.append(\"http://satoshi.nakamotoinstitute.org\" + link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://satoshi.nakamotoinstitute.org/posts/p2pfoundation/',\n",
       " 'http://satoshi.nakamotoinstitute.org/posts/bitcointalk/',\n",
       " 'http://satoshi.nakamotoinstitute.org/posts/p2pfoundation/1/',\n",
       " 'http://satoshi.nakamotoinstitute.org/posts/p2pfoundation/2/',\n",
       " 'http://satoshi.nakamotoinstitute.org/posts/p2pfoundation/3/',\n",
       " 'http://satoshi.nakamotoinstitute.org/posts/bitcointalk/5/',\n",
       " 'http://satoshi.nakamotoinstitute.org/posts/bitcointalk/6/']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test links to make sure we have our URLs properly defined\n",
    "links[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the links for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define crawler function\n",
    "def crawler(links):\n",
    "    \"pull in links list object, scrape text, create txt files\"\n",
    "    # pull in links list\n",
    "    links = links\n",
    "    # set file name counter\n",
    "    file_count = 1\n",
    "    # interate through links\n",
    "    for link in links:\n",
    "        # repeat the process above for collecting the list of links\n",
    "        # this time we're gathering the body of text from each page\n",
    "        post_raw = requests.get(link)\n",
    "        post_soup = BeautifulSoup((post_raw.text), \"html5lib\")\n",
    "        text_body = post_soup.find(class_='col-sm-6 col-sm-offset-3 col-md-6 col-md-offset-3 col-lg-6 col-lg-offset-3')\n",
    "        text = text_body.find_all('div')\n",
    "        post_content = text[1]\n",
    "        # create an output file, write the content of a link to the file, close the file, increase the file count for the next link\n",
    "        output_file = open('posts/post_%s.txt' % str(file_count), 'x')\n",
    "        output_file.write(post_content.get_text())\n",
    "        output_file.close()\n",
    "        file_count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dev containing the links collected above have preceeding and proceeding meta links that we don't need to scrape\n",
    "# setting the list index below excludes those links\n",
    "links_no_ends = links[2:545]\n",
    "len(links_no_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the crawler to generate the documents (should be about 543)\n",
    "crawler(links_no_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
